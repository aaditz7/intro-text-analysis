{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4 Submission\n",
    "### Rohan and Aadit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've created a utils.py file for frequently reused functionality -- you can import from it like so\n",
    "from utils import load_pausanias\n",
    "\n",
    "pausanias_df = load_pausanias('eng') # you can use `load_pausanias('eng')` to load the English version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_iliad\n",
    "\n",
    "iliad_df = load_iliad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating co-occurrences in Greek, it is generally insufficient to use the L and R windows that Brezina uses for English (@Brezina2018 67â€“70). Instead, we'll look for a dependency relationship between the **node** and its **collocates**. Below, you can see that we can access the dependencies of a token through its `children` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"token: 'the, the', dependencies: []\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token = pausanias_df['nlp_docs'][0][1]\n",
    "\n",
    "# we use a list comprehension to evaluate the generator at `test_token.children`\n",
    "f\"token: '{test_token}, {test_token.lemma_}', dependencies: {[(c, c.lemma_) for c in test_token.children]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mu function\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def expected_frequency_of_collocation(df: pd.DataFrame, node: str, collocate: str, window_size: int = 1):\n",
    "    \"\"\"\n",
    "    `node` and `collocate` should be the string representations\n",
    "    of the associated lemmata\n",
    "    \"\"\"\n",
    "\n",
    "    lemmata = [t.lemma_ for t in df['nlp_docs'].explode()]\n",
    "    counter = Counter(lemmata)\n",
    "    node_count = counter[node]\n",
    "    collocate_count = counter[collocate]\n",
    "\n",
    "    return (node_count * collocate_count * window_size) / len(lemmata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltaP(node, collocate, df):    \n",
    "\n",
    "    def count_ngram_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "        lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "        indexes = [i for i, lemma in enumerate(lemmata) if lemma == w1]\n",
    "\n",
    "        cooccurrences = 0\n",
    "\n",
    "        for i in indexes:\n",
    "            left = max(i - l_size, 0)\n",
    "            right = min(i + r_size + 1, len(lemmata))\n",
    "            window = lemmata[left:right]\n",
    "\n",
    "            if w2 in window:\n",
    "                cooccurrences += 1\n",
    "                \n",
    "        return cooccurrences\n",
    "\n",
    "    df['o11_temple_a_ngrams'] = df['nlp_docs'].apply(count_ngram_collocations, args=(node, collocate))\n",
    "\n",
    "    o11 = df['o11_temple_a_ngrams'].sum()\n",
    "\n",
    "    all_tokens = df['nlp_docs'].explode()\n",
    "\n",
    "    r1 = len([t for t in all_tokens if t.lemma_ == node])\n",
    "    r2 = len(all_tokens) - r1\n",
    "    c1 = len([t for t in all_tokens if t.lemma_ == collocate])\n",
    "    o21 = c1 - o11\n",
    "\n",
    "    return (o11 / r1) - (o21 / r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're also accessing the `lemma_` property here. Because Greek is heavily inflected, we'll tend to focus on collocations of lemmata, rather than types -- but you might find in your own work that it is interesting to look at type collocations instead. Just be sure to note which kind of \"word\" you're examining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of co-occurrence\n",
    "\n",
    "The frequency of co-occurrence reports the presence of both a **node** (`w1`) and a **collocate** (`w2`). Given a DataFrame like `pausanias_df`, we can calculate the frequency of co-occurrence in two different ways. \n",
    "\n",
    "We can either count when the collocate is a dependency of the node, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pausanias: \n",
      "Observed Freq:  322\n",
      "Mu:  8.314623403386307\n",
      "Mutual Information:  3.0556509205935973\n",
      "Delta P:  0.6868650246447927\n"
     ]
    }
   ],
   "source": [
    "print('Pausanias: ')\n",
    "node = 'god'\n",
    "collocate = 'the' # the\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "pausanias_df['dependencies'] = pausanias_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = pausanias_df[pausanias_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(pausanias_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "import math\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_1 = deltaP(node, collocate, pausanias_df)\n",
    "print('Delta P: ', deltaP_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iliad: \n",
      "Observed Freq:  179\n",
      "Mu:  6.554758580371991\n",
      "Mutual Information:  2.712542645235118\n",
      "Delta P:  0.4037241526145461\n"
     ]
    }
   ],
   "source": [
    "print('Iliad: ')\n",
    "node = 'god'\n",
    "collocate = 'the' # the\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "iliad_df['dependencies'] = iliad_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = iliad_df[iliad_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(iliad_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "import math\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_1 = deltaP(node, collocate, iliad_df)\n",
    "print('Delta P: ', deltaP_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Freq:  15\n",
      "Mu:  1.7737622063788958\n",
      "Mutual Information:  0.8268126124164578\n",
      "Delta P:  0.01462683309133956\n"
     ]
    }
   ],
   "source": [
    "node = 'god' \n",
    "collocate = 'a' \n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "pausanias_df['dependencies'] = pausanias_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = pausanias_df[pausanias_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "observed_freq\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(pausanias_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_2 = deltaP(node, collocate, pausanias_df)\n",
    "print('Delta P: ', deltaP_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iliad: \n",
      "Observed Freq:  40\n",
      "Mu:  8.611590793436276\n",
      "Mutual Information:  3.106279766984668\n",
      "Delta P:  0.09968310338954181\n"
     ]
    }
   ],
   "source": [
    "print('Iliad: ')\n",
    "node = 'god'\n",
    "collocate = 'a' # the\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "iliad_df['dependencies'] = iliad_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = iliad_df[iliad_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(iliad_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "import math\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_1 = deltaP(node, collocate, iliad_df)\n",
    "print('Delta P: ', deltaP_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Freq:  17\n",
      "Mu:  0.8190066925627196\n",
      "Mutual Information:  -0.2880528539031336\n",
      "Delta P:  0.009484953336988497\n"
     ]
    }
   ],
   "source": [
    "node = 'god'  \n",
    "collocate = 'of' # mother\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "pausanias_df['dependencies'] = pausanias_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = pausanias_df[pausanias_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "observed_freq\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(pausanias_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_3 = deltaP(node, collocate, pausanias_df)\n",
    "print('Delta P: ', deltaP_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iliad: \n",
      "Observed Freq:  15\n",
      "Mu:  0.9944817380983576\n",
      "Mutual Information:  -0.007983216132855285\n",
      "Delta P:  0.032784619315278485\n"
     ]
    }
   ],
   "source": [
    "print('Iliad: ')\n",
    "node = 'god'\n",
    "collocate = 'of' # the\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "iliad_df['dependencies'] = iliad_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = iliad_df[iliad_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(iliad_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "import math\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_1 = deltaP(node, collocate, iliad_df)\n",
    "print('Delta P: ', deltaP_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Freq:  8\n",
      "Mu:  1.1282628849552394\n",
      "Mutual Information:  0.1741032544687521\n",
      "Delta P:  0.013209740295223218\n"
     ]
    }
   ],
   "source": [
    "node = 'god' # temple\n",
    "collocate = 'in' # god\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "pausanias_df['dependencies'] = pausanias_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = pausanias_df[pausanias_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "observed_freq\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(pausanias_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_4 = deltaP(node, collocate, pausanias_df)\n",
    "print('Delta P: ', deltaP_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iliad: \n",
      "Observed Freq:  6\n",
      "Mu:  0.9205443293085306\n",
      "Mutual Information:  -0.11944089788176607\n",
      "Delta P:  0.009850962357349684\n"
     ]
    }
   ],
   "source": [
    "print('Iliad: ')\n",
    "node = 'god'\n",
    "collocate = 'in' # the\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "iliad_df['dependencies'] = iliad_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = iliad_df[iliad_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(iliad_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "import math\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_1 = deltaP(node, collocate, iliad_df)\n",
    "print('Delta P: ', deltaP_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Freq:  1\n",
      "Mu:  2.341525372775373\n",
      "Mutual Information:  1.2274486711691057\n",
      "Delta P:  0.0012806986548452477\n"
     ]
    }
   ],
   "source": [
    "node= 'god' # priest\n",
    "collocate = 'great'  # god\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "pausanias_df['dependencies'] = pausanias_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = pausanias_df[pausanias_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "observed_freq\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(pausanias_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_5 = deltaP(node, collocate, pausanias_df)\n",
    "print('Delta P: ', deltaP_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iliad: \n",
      "Observed Freq:  3\n",
      "Mu:  3.218633075579945\n",
      "Mutual Information:  1.686448118843056\n",
      "Delta P:  0.004544966785896781\n"
     ]
    }
   ],
   "source": [
    "print('Iliad: ')\n",
    "node = 'god'\n",
    "collocate = 'great' # the\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "iliad_df['dependencies'] = iliad_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_freq = iliad_df[iliad_df['dependencies'] > 0].shape[0]\n",
    "\n",
    "print('Observed Freq: ', observed_freq)\n",
    "\n",
    "expected_freq = expected_frequency_of_collocation(iliad_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_freq / expected_freq\n",
    "\n",
    "print('Mu: ', mu_deps)\n",
    "\n",
    "import math\n",
    "\n",
    "mutual_info = math.log(mu_deps, 2)\n",
    "print('Mutual Information: ', mutual_info)\n",
    "\n",
    "deltaP_1 = deltaP(node, collocate, iliad_df)\n",
    "print('Delta P: ', deltaP_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can count when the collocate and node co-occur within a given window, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment in your own notebooks by adjusting the `l_size` and `r_size` args passed to the `count_ngram_collocations` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "You may work in groups for these exercises. They are due at the beginning of next class. You can submit them as a link to a Colab notebook or GitHub CodeSpace.\n",
    "\n",
    "0. Use the corpora that you assembled last week (Pausanias++):\n",
    "1. Using programming techniques from the course so far, find other potential collocates for a word of your choice (indicative verb)\n",
    "2. Calculate the Î¼ and Mutual Information scores for at least 5 of these collocate pairs. How do your results change depending on your definition of a collocation? What might these changes mean? (Write your answers to these questions down.)\n",
    "3. Calculate the Delta P for these same five pairs. Do any results stand out? Why? What might they tell us about your corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
